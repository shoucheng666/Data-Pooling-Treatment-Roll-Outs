{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from copy import deepcopy\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom loss function \n",
    "def custom_loss(output, target, treatment):\n",
    "    error = torch.sum(output*treatment,dim=1)\n",
    "    loss = torch.mean((target - error)**2)\n",
    "    return loss\n",
    "\n",
    "# function to calculate the Phi value\n",
    "def Phi(x,theta,Gamma,treatment,y,device):\n",
    "    with torch.no_grad():\n",
    "        theta.eval()\n",
    "        #device = torch.device('mps')\n",
    "        a = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "        output = theta(a).detach().cpu().numpy()\n",
    "        term1 = np.dot(output,np.array([0,1]))\n",
    "        term2 = (np.array([0,1]).reshape(1,2)) @ (np.linalg.inv(Gamma)) @ (2*(np.dot(output,treatment) - y)*treatment.reshape(2,1))\n",
    "\n",
    "    return term1 - term2[0,0]\n",
    "\n",
    "# function to split the data into S parts\n",
    "def data_split(S,Hist_feature,Hist_treatment,Hist_label):\n",
    "    N = Hist_feature.shape[1]\n",
    "    number_per_split = int(N/S)\n",
    "    data_index = np.arange(N)\n",
    "    np.random.shuffle(data_index)\n",
    "    feature_list = []\n",
    "    treatment_list = []\n",
    "    label_list = []\n",
    "\n",
    "    for i in range(S):\n",
    "        feature_list.append(Hist_feature[:,data_index[i*number_per_split:(i+1)*number_per_split],:])\n",
    "        treatment_list.append(Hist_treatment[:,data_index[i*number_per_split:(i+1)*number_per_split],:])\n",
    "        label_list.append(Hist_label[:,data_index[i*number_per_split:(i+1)*number_per_split]])\n",
    "    \n",
    "    return feature_list,treatment_list,label_list\n",
    "\n",
    "# A simple two-layer fully connected neural network\n",
    "class TwoLayerFCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(TwoLayerFCN, self).__init__()\n",
    "        # first layer: input layer to hidden layer\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        # second layer: hidden layer to output layer\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        # activation function\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # forward pass through the network\n",
    "        x = self.fc1(x)          \n",
    "        x = self.activation(x)   \n",
    "        x = self.fc2(x)          \n",
    "        return x\n",
    "    \n",
    "# Function to train the model\n",
    "def train_model(S,s,feature_list,treatment_list,label_list,dim_feature,K,device):\n",
    "    train_data_list = []\n",
    "    train_label_list = []\n",
    "    train_treatment_list = []\n",
    "    for i in range(S):\n",
    "        if i != s:\n",
    "            train_data_list.append(feature_list[i])\n",
    "            train_label_list.append(label_list[i])\n",
    "            train_treatment_list.append(treatment_list[i])\n",
    "    train_data_array = np.concatenate(train_data_list, axis=1)\n",
    "    train_label_array = np.concatenate(train_label_list, axis=1)\n",
    "    train_treatment_array = np.concatenate(train_treatment_list, axis=1)\n",
    "   \n",
    "    \n",
    "    #estmate the parameter\n",
    "    model_list = []\n",
    "    for k in range(K):\n",
    "        data = train_data_array[k]\n",
    "        label = train_label_array[k]\n",
    "        treatment = train_treatment_array[k]\n",
    "        data = torch.tensor(data, dtype=torch.float32).to(device)\n",
    "        label = torch.tensor(label, dtype=torch.float32).to(device)\n",
    "        treatment = torch.tensor(treatment, dtype=torch.float32).to(device)\n",
    "        model = TwoLayerFCN(dim_feature, 10, 2).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        for epoch in range(10):\n",
    "            batch_size = 64\n",
    "            index_list = np.random.permutation(data.shape[0])\n",
    "            data_list = []\n",
    "            label_list = []\n",
    "            treatment_list = []\n",
    "            number_parts = int(data.shape[0]/batch_size)\n",
    "            for i in range(number_parts):\n",
    "                if i < number_parts - 1:\n",
    "                    data_list.append(data[index_list[i*batch_size:(i+1)*batch_size],:])\n",
    "                    label_list.append(label[index_list[i*batch_size:(i+1)*batch_size]])\n",
    "                    treatment_list.append(treatment[index_list[i*batch_size:(i+1)*batch_size]])\n",
    "                else:\n",
    "                    data_list.append(data[index_list[i*batch_size:],:])\n",
    "                    label_list.append(label[index_list[i*batch_size:]])\n",
    "                    treatment_list.append(treatment[index_list[i*batch_size:]])\n",
    "            for i in range(number_parts):\n",
    "                model.train()\n",
    "                output = model(data_list[i])\n",
    "                #print(output.shape,treatment.shape)\n",
    "                loss = custom_loss(output, label_list[i], treatment_list[i])\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        model_list.append(model)\n",
    "    \n",
    "    temp_reshaped = train_treatment_array.reshape(K, train_treatment_array.shape[1], -1, 1)\n",
    "    outer_product = temp_reshaped @ temp_reshaped.transpose(0, 1, 3, 2) \n",
    "    estimated_Gamma = 2 * outer_product.mean(axis=1)  \n",
    "    \n",
    "    return model_list, estimated_Gamma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1, Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_feature = 4 # number of features\n",
    "K = 100 # number of experiments\n",
    "Z = norm.ppf(0.975) # 95% confidence interval\n",
    "N = 100 # number of samples per experiment\n",
    "\n",
    "T = 1000 # number of time steps\n",
    "S = 2 # number of splits\n",
    "number_per_split = int(N/S) # number of samples per split\n",
    "std = 3 # standard deviation of noise\n",
    "\n",
    "\n",
    "true_ate_array  = np.zeros((T,K))\n",
    "estimated_phi = np.zeros((T,K,S,number_per_split))\n",
    "b_k = np.zeros((T,K))\n",
    "device = torch.device('cpu')\n",
    "for t in tqdm(range(T)):\n",
    "    constant_linear = np.random.uniform(-0.3,0.5,(K,dim_feature))\n",
    "\n",
    "    # linear true function\n",
    "    cofficent_linear = np.random.uniform(-0.3,0.5,(K,dim_feature))\n",
    "\n",
    "    true_ate = np.sum(cofficent_linear[:,:]*0.5, axis=1)\n",
    "    true_ate_array[t,:] = true_ate\n",
    "\n",
    "\n",
    "    #generate data\n",
    "    Hist_feature = np.random.uniform(0,1,(K,N,dim_feature))\n",
    "\n",
    "    Hist_treatment = np.ones((K, N, 2))\n",
    "    Hist_treatment[:, int(N/2):, 1] = 0\n",
    "\n",
    "    temp_feature = np.zeros((K, N, dim_feature + 1))\n",
    "    temp_feature[:,:,0] = Hist_feature[:,:,0]\n",
    "    temp_feature[:,:,1] = Hist_treatment[:,:,1]\n",
    "    temp_feature[:,:,2:] = Hist_feature[:,:,1:]\n",
    "    \n",
    "    for k in range(K):\n",
    "        a = temp_feature[k].T@temp_feature[k]\n",
    "        b = np.linalg.inv(a + np.eye(dim_feature + 1)*10**(-5))\n",
    "        b_k[t,k] = N*b[1,1]\n",
    "\n",
    "\n",
    "\n",
    "    # constant linear true function\n",
    "    Hist_label = np.zeros((K, N))\n",
    "    for i in range(N):\n",
    "        Hist_label[:,i] = np.sum(Hist_feature[:,i,:]*constant_linear, axis=1) + np.sum(Hist_feature[:,i,:]*cofficent_linear, axis=1)*Hist_treatment[:,i,1] + np.random.normal(0,std,K)\n",
    "\n",
    "    #split data\n",
    "    feature_list,treatment_list,label_list = data_split(S,Hist_feature,Hist_treatment,Hist_label)\n",
    "\n",
    "    #train model\n",
    "    for s in range(S):\n",
    "        model_list,estimated_Gamma = train_model(S,s,feature_list,treatment_list,label_list,dim_feature,K,device)\n",
    "        for k in range(K):\n",
    "            feature_ate = feature_list[s][k]\n",
    "            treatment_ate = treatment_list[s][k]\n",
    "            label_ate = label_list[s][k]\n",
    "            theta = model_list[k]\n",
    "            Gamma = estimated_Gamma[k]\n",
    "            for i in range(feature_ate.shape[0]):\n",
    "                phi_value = Phi(feature_ate[i],theta,Gamma,treatment_ate[i],label_ate[i],device)\n",
    "                estimated_phi[t,k,s,i] = phi_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = np.zeros((3,T))\n",
    "accuracy = np.zeros((2,1000))\n",
    "recall = np.zeros((2,1000))\n",
    "FPR = np.zeros((2,1000))\n",
    "precision = np.zeros((2,1000))\n",
    "\n",
    "for t in range(T):\n",
    "    hat_ATE = np.mean(estimated_phi[t],axis=(1,2))\n",
    "    estimated_variance1 = np.mean((estimated_phi[t] - np.tile(hat_ATE.reshape(K,1,1),(S,number_per_split)))**2,axis=(1,2))\n",
    "    estimated_variance = np.mean((estimated_phi[t] - np.tile(hat_ATE.reshape(K,1,1),(S,number_per_split)))**2)\n",
    "    true_tao = true_ate_array[t]\n",
    "    optimal_cost = np.sum(true_tao[np.argwhere(true_tao>0)])\n",
    "    if optimal_cost == 0:\n",
    "        continue\n",
    "    cost[0,t] = np.sum(true_tao[np.argwhere(hat_ATE>(Z*np.sqrt(estimated_variance1))/np.sqrt(N))])/optimal_cost\n",
    "    decision1 = np.argwhere(hat_ATE>(Z*np.sqrt(estimated_variance1))/np.sqrt(N))\n",
    "\n",
    "    #DRT\n",
    "    anchor_tau = np.mean(hat_ATE)\n",
    "    beta = estimated_variance/(np.mean((hat_ATE - anchor_tau)**2) - estimated_variance/N) + Z*np.sqrt(N*estimated_variance)/anchor_tau\n",
    "    beta = max(0,beta)\n",
    "    theta = N/(N+beta)\n",
    "    hat_ATE_shrunken = theta*hat_ATE + (1-theta)*anchor_tau\n",
    "    cost[1,t] = np.sum(true_tao[np.argwhere(hat_ATE_shrunken>(theta*Z*np.sqrt(estimated_variance1))/np.sqrt(N))])/optimal_cost\n",
    "    decision2 = np.argwhere(hat_ATE_shrunken>(theta*Z*np.sqrt(estimated_variance1))/np.sqrt(N))\n",
    "\n",
    "    #DRT-P\n",
    "    estimated_variance_p = np.mean(estimated_variance1/b_k[t])\n",
    "    theta_list = np.zeros(K)\n",
    "    for k in range(K):\n",
    "        beta_p = estimated_variance_p*b_k[t,k]/(np.mean((hat_ATE - anchor_tau)**2) - (estimated_variance_p*np.mean(b_k[t]))/N) + Z*b_k[t,k]*np.sqrt(N*estimated_variance_p)/anchor_tau\n",
    "        beta_p = max(0,beta_p)\n",
    "        theta_list[k] = N/(N+beta_p)\n",
    "    hat_ATE_shrunken_p = theta_list*hat_ATE + (1-theta_list)*anchor_tau\n",
    "\n",
    "    cost[2,t] = np.sum(true_tao[np.argwhere(hat_ATE_shrunken_p>(theta_list*Z*np.sqrt(estimated_variance1))/np.sqrt(N))])/optimal_cost\n",
    "\n",
    "\n",
    "    for k in range(K):\n",
    "        if (true_tao[k] < 0 and k not in decision1) or (true_tao[k] > 0 and k in decision1):\n",
    "            accuracy[0,t] += 1\n",
    "        if (true_tao[k] < 0 and k not in decision2) or (true_tao[k] > 0 and k in decision2):\n",
    "            accuracy[1,t] += 1\n",
    "        if true_tao[k] > 0 and k in decision1:\n",
    "            recall[0,t] += 1\n",
    "        if true_tao[k] > 0 and k in decision2:\n",
    "            recall[1,t] += 1\n",
    "        if true_tao[k] < 0 and k in decision1:\n",
    "            FPR[0,t] += 1\n",
    "        if true_tao[k] <0 and k in decision2:\n",
    "            FPR[1,t] += 1\n",
    "    if recall[0,t] +FPR[0,t] == 0:\n",
    "        precision[0,t] = 1\n",
    "    else:\n",
    "        precision[0,t] = recall[0,t]/(recall[0,t] +FPR[0,t] )\n",
    "    if recall[1,t] +FPR[1,t] == 0:\n",
    "        precision[1,t] = 1\n",
    "    else:\n",
    "        precision[1,t] = recall[1,t]/(recall[1,t] +FPR[1,t] )\n",
    "    accuracy[:,t] = accuracy[:,t]/K\n",
    "    recall[:,t] = recall[:,t]/(len(np.argwhere(true_tao>0)))\n",
    "    FPR[:,t] = FPR[:,t]/(len(np.argwhere(true_tao<0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (8,6))\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "data = [cost[1,:] - cost[0,:],accuracy[1,:] - accuracy[0,:],recall[1,:] - recall[0,:],FPR[0,:] - FPR[1,:],precision[1,:] - precision[0,:]]\n",
    "\n",
    "bp = plt.boxplot(data,showfliers=False,showmeans=True,patch_artist=True)\n",
    "\n",
    "colors = [ '#9DB4CE','#EDA1A4','#FCB462','#7BC4C5','#893E81']\n",
    "colors1 = ['#A3A5A6' ,'#A3A5A6','#FFE8CE','#D9EEEE','#DCA5C3']\n",
    "\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_edgecolor(color)\n",
    "    \n",
    "for whisker, color in zip(bp['whiskers'], [colors[i // 2] for i in range(len(bp['whiskers']))]):\n",
    "    whisker.set_color(color)\n",
    "\n",
    "for cap, color in zip(bp['caps'], [colors[i // 2] for i in range(len(bp['caps']))]):\n",
    "    cap.set_color(color)\n",
    "\n",
    "for median, color in zip(bp['medians'], colors1):\n",
    "    median.set_color(color)\n",
    "\n",
    "for flier, color in zip(bp['fliers'], [colors[i // 2] for i in range(len(bp['fliers']))]):\n",
    "    flier.set_markerfacecolor(color)\n",
    "    flier.set_markeredgecolor(color)\n",
    "\n",
    "plt.ylim(-1.1,1)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.xticks([1, 2,3,4,5], ['OR','Accuracy','Recall','Specificity','Precision'],fontsize=15)\n",
    "\n",
    "\n",
    "plt.savefig('boxplot_basic_dml.png',dpi = 300,bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2, Cost with varianc of noise term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_feature = 4\n",
    "K = 100 \n",
    "#K = 5\n",
    "Z = norm.ppf(0.975)\n",
    "N = 100\n",
    "\n",
    "T = 1000\n",
    "S = 2\n",
    "number_per_split = int(N/S)\n",
    "noise_list = [1,2,3,4,5] # standard deviation of noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_ate_array  = np.zeros((T,K))\n",
    "estimated_phi = np.zeros((len(noise_list),T,K,S,number_per_split))\n",
    "b_k = np.zeros((T,K))\n",
    "device = torch.device('cpu')\n",
    "for t in tqdm(range(T)):\n",
    "    constant_linear = np.random.uniform(-0.3,0.5,(K,dim_feature))\n",
    "\n",
    "    # linear true function\n",
    "    cofficent_linear = np.random.uniform(-0.3,0.5,(K,dim_feature))\n",
    "\n",
    "    true_ate = np.sum(cofficent_linear[:,:]*0.5, axis=1)\n",
    "    true_ate_array[t,:] = true_ate\n",
    "\n",
    "\n",
    "    #generate data\n",
    "    Hist_feature = np.random.uniform(0,1,(K,N,dim_feature))\n",
    "\n",
    "    Hist_treatment = np.ones((K, N, 2))\n",
    "    Hist_treatment[:, int(N/2):, 1] = 0\n",
    "\n",
    "    temp_feature = np.zeros((K, N, dim_feature + 1))\n",
    "    temp_feature[:,:,0] = Hist_feature[:,:,0]\n",
    "    temp_feature[:,:,1] = Hist_treatment[:,:,1]\n",
    "    temp_feature[:,:,2:] = Hist_feature[:,:,1:]\n",
    "    \n",
    "    for k in range(K):\n",
    "        a = temp_feature[k].T@temp_feature[k]\n",
    "        b = np.linalg.inv(a + np.eye(dim_feature + 1)*10**(-5))\n",
    "        b_k[t,k] = N*b[1,1]\n",
    "\n",
    "\n",
    "    for id,std in enumerate(noise_list):\n",
    "    # constant linear true function\n",
    "        Hist_label = np.zeros((K, N))\n",
    "        for i in range(N):\n",
    "            Hist_label[:,i] = np.sum(Hist_feature[:,i,:]*constant_linear, axis=1) + np.sum(Hist_feature[:,i,:]*cofficent_linear, axis=1)*Hist_treatment[:,i,1] + np.random.normal(0,std,K)\n",
    "\n",
    "        #split data\n",
    "        feature_list,treatment_list,label_list = data_split(S,Hist_feature,Hist_treatment,Hist_label)\n",
    "\n",
    "        #train model\n",
    "        for s in range(S):\n",
    "            model_list,estimated_Gamma = train_model(S,s,feature_list,treatment_list,label_list,dim_feature,K,device)\n",
    "            for k in range(K):\n",
    "                feature_ate = feature_list[s][k]\n",
    "                treatment_ate = treatment_list[s][k]\n",
    "                label_ate = label_list[s][k]\n",
    "                theta = model_list[k]\n",
    "                Gamma = estimated_Gamma[k]\n",
    "                for i in range(feature_ate.shape[0]):\n",
    "                    phi_value = Phi(feature_ate[i],theta,Gamma,treatment_ate[i],label_ate[i],device)\n",
    "                    estimated_phi[id,t,k,s,i] = phi_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = np.zeros((len(noise_list),3,T))\n",
    "accuracy = np.zeros((2,1000))\n",
    "recall = np.zeros((2,1000))\n",
    "FPR = np.zeros((2,1000))\n",
    "precision = np.zeros((2,1000))\n",
    "for i in range(len(noise_list)):\n",
    "    for t in range(T):\n",
    "        hat_ATE = np.mean(estimated_phi[i,t],axis=(1,2))\n",
    "        estimated_variance1 = np.mean((estimated_phi[i,t] - np.tile(hat_ATE.reshape(K,1,1),(S,number_per_split)))**2,axis=(1,2))\n",
    "        estimated_variance = np.mean((estimated_phi[i,t] - np.tile(hat_ATE.reshape(K,1,1),(S,number_per_split)))**2)\n",
    "        true_tao = true_ate_array[t]\n",
    "        optimal_cost = np.sum(true_tao[np.argwhere(true_tao>0)])\n",
    "        if optimal_cost == 0:\n",
    "            continue\n",
    "        cost[i,0,t] = np.sum(true_tao[np.argwhere(hat_ATE>(Z*np.sqrt(estimated_variance1))/np.sqrt(N))])/optimal_cost\n",
    "        decision1 = np.argwhere(hat_ATE>(Z*np.sqrt(estimated_variance1))/np.sqrt(N))\n",
    "\n",
    "        #DRT\n",
    "        anchor_tau = np.mean(hat_ATE)\n",
    "        beta = estimated_variance/(np.mean((hat_ATE - anchor_tau)**2) - estimated_variance/N) + Z*np.sqrt(N*estimated_variance)/anchor_tau\n",
    "        beta = max(0,beta)\n",
    "        theta = N/(N+beta)\n",
    "        hat_ATE_shrunken = theta*hat_ATE + (1-theta)*anchor_tau\n",
    "        cost[i,1,t] = np.sum(true_tao[np.argwhere(hat_ATE_shrunken>(theta*Z*np.sqrt(estimated_variance1))/np.sqrt(N))])/optimal_cost\n",
    "        decision2 = np.argwhere(hat_ATE_shrunken>(theta*Z*np.sqrt(estimated_variance1))/np.sqrt(N))\n",
    "\n",
    "        #DRT-P\n",
    "        estimated_variance_p = np.mean(estimated_variance1/b_k[t])\n",
    "        theta_list = np.zeros(K)\n",
    "        for k in range(K):\n",
    "            beta_p = estimated_variance_p*b_k[t,k]/(np.mean((hat_ATE - anchor_tau)**2) - (estimated_variance_p*np.mean(b_k[t]))/N) + Z*b_k[t,k]*np.sqrt(N*estimated_variance_p)/anchor_tau\n",
    "            beta_p = max(0,beta_p)\n",
    "            theta_list[k] = N/(N+beta_p)\n",
    "        hat_ATE_shrunken_p = theta_list*hat_ATE + (1-theta_list)*anchor_tau\n",
    "\n",
    "        cost[i,2,t] = np.sum(true_tao[np.argwhere(hat_ATE_shrunken_p>(theta_list*Z*np.sqrt(estimated_variance1))/np.sqrt(N))])/optimal_cost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
