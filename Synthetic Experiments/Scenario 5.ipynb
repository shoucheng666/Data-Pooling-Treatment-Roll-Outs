{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from numpy import linalg as LA\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.optimize import curve_fit\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import os\n",
    "import os.path\n",
    "import argparse\n",
    "from torch.autograd import Variable\n",
    "import statsmodels.api as sm\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_p(value, round_digits):\n",
    "    return format(round(value,round_digits), \".\"+str(round_digits)+\"f\")\n",
    "\n",
    "def generate_x(d_c, n):\n",
    "    x = np.random.uniform(0,1, [n, d_c])\n",
    "  \n",
    "    return x\n",
    "\n",
    "def generate_t(t_combo, t_dist, n):\n",
    "    return t_combo[np.random.choice(np.shape(t_dist)[0], n, p=t_dist),]\n",
    "\n",
    "\n",
    "def powerset(s):\n",
    "    x = len(s)\n",
    "    masks = [1 << i for i in range(x)]\n",
    "    for i in range(1 << x):\n",
    "        yield [ss for mask, ss in zip(masks, s) if i & mask]\n",
    "\n",
    "def calculate_mse(loader, is_gpu, net):\n",
    "    \"\"\"Calculate accuracy.\n",
    "\n",
    "    Args:\n",
    "        loader (torch.utils.data.DataLoader): training / test set loader\n",
    "        is_gpu (bool): whether to run on GPU\n",
    "    Returns:\n",
    "        tuple: (overall accuracy, class level accuracy)\n",
    "    \"\"\"\n",
    "    cnt = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    for data in loader:\n",
    "        inputs, labels = data\n",
    "        if is_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        b,outputs = net(inputs)\n",
    "        \n",
    "        \n",
    "        cnt += labels.size(0)\n",
    "        total_loss += sum((outputs-labels)**2)\n",
    "\n",
    "    return total_loss/float(cnt)\n",
    "\n",
    "def plot_dev(loader, is_gpu, net):\n",
    "    cnt = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    real_y = []\n",
    "    pred_y = []\n",
    "\n",
    "    for data in loader:\n",
    "        inputs, labels = data\n",
    "        if is_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        real_y.append(labels.tolist())\n",
    "        pred_y.append(outputs.tolist())\n",
    "        \n",
    "        cnt += labels.size(0)\n",
    "        total_loss += sum((outputs-labels)**2)\n",
    "    real_y = [x for sublist in real_y for x in sublist]\n",
    "    pred_y = [x for sublist in pred_y for x in sublist]\n",
    "    print('MSELoss= ', total_loss/float(cnt))\n",
    "    plt.scatter(real_y, pred_y)\n",
    "    plt.xlabel('real y')\n",
    "    plt.ylabel('pred y')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# hyperparameters settings\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='learning rate')\n",
    "parser.add_argument('--wd', type=float, default=5e-4, help='weight decay')#lr/(c+wd)\n",
    "parser.add_argument('--epochs', type=int, default=50,\n",
    "                    help='number of epochs to train')\n",
    "parser.add_argument('--batch_size_train', type=int,\n",
    "                    default=1000, help='training set input batch size')\n",
    "parser.add_argument('--batch_size_test', type=int,\n",
    "                    default=1000, help='test set input batch size')\n",
    "parser.add_argument('--is_gpu', type=bool, default=False,\n",
    "                    help='whether training using GPU')\n",
    "import sys\n",
    "sys.argv=['']\n",
    "del sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 4 #number of experiments\n",
    "d_c = 4 #number of features\n",
    "lr = 0.05\n",
    "\n",
    "sample_size = m + 100\n",
    "reg_term = 0.0005\n",
    "#reg_loss = 0.001\n",
    "reg_loss = 0\n",
    "train_epochs = 2000\n",
    "test_thres = 0.3\n",
    "n_cnver_test = 10000\n",
    "\n",
    "feature_list = []\n",
    "for i in range(d_c):\n",
    "    feature_list.append(str('x')+str(i+1))\n",
    "t_list = []\n",
    "for i in range(m+1):\n",
    "    t_list.append(str('t')+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def powerset(s):\n",
    "    x = len(s)\n",
    "    masks = [1 << i for i in range(x)]\n",
    "    for i in range(1 << x):\n",
    "        yield [ss for mask, ss in zip(masks, s) if i & mask]\n",
    "def calculate_mse(loader, is_gpu, net):\n",
    "    \"\"\"Calculate accuracy.\n",
    "\n",
    "    Args:\n",
    "        loader (torch.utils.data.DataLoader): training / test set loader\n",
    "        is_gpu (bool): whether to run on GPU\n",
    "    Returns:\n",
    "        tuple: (overall accuracy, class level accuracy)\n",
    "    \"\"\"\n",
    "    cnt = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    for data in loader:\n",
    "        inputs, labels = data\n",
    "        if is_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        \n",
    "        cnt += labels.size(0)\n",
    "        total_loss += sum((outputs-labels)**2)\n",
    "\n",
    "    return total_loss/float(cnt)\n",
    "\n",
    "all_combo = list(powerset(list(np.arange(1, m+1))))\n",
    "t_star_all = []\n",
    "for i in all_combo:\n",
    "    t = np.zeros(m+1)\n",
    "    t[0] = 1\n",
    "    t[i] = 1\n",
    "    t_star_all.append(t)\n",
    "t_star_all = np.int16(t_star_all)\n",
    "t_dist_obs = (1/(2**m))*np.ones(2**m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN_asig(nn.Module):\n",
    "    def __init__(self, d_c, m):\n",
    "        \"\"\"Feedforward Neural Network for assignment.\"\"\"\n",
    "        super(FNN_asig, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(d_c, 10, bias=False),  # Set d_c to 46 (number of input features)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(10, m + 1)  # Output size depends on m (e.g., treatments)\n",
    "        )\n",
    "        #self.siglayer = nn.Sigmoid()\n",
    "        self.layer3 = nn.Linear(1, 1, bias=False)\n",
    "        #self.constant = nn.Parameter(torch.randn(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b = self.layer1(x[:, :d_c])  # Apply first layer to feature inputs\n",
    "        u = torch.sum(b * x[:, d_c:], 1)  # Element-wise multiplication with treatments\n",
    "        #u_sigmoid_scaled = self.siglayer(u)*self.constant\n",
    "        u = u.unsqueeze(1)\n",
    "        u = self.layer3(u)\n",
    "        return torch.reshape(u, (-1,))\n",
    "\n",
    "    \n",
    "def get_activation(name,activation):\n",
    "        def hook(model, input, output):\n",
    "            activation[name] = output.detach()\n",
    "        return hook\n",
    "\n",
    "def generate_y_true(coef, c, d, x, t, n,std):\n",
    "    y = np.zeros(n)\n",
    "    y_error = np.random.normal(0,std,n)\n",
    "    #y_error = 0.15*np.random.uniform(-1, 1, n)\n",
    "    for i in range(n):\n",
    "        y[i] = c/(1+np.exp(-((x[i].dot(coef))).dot(t[i]))) + d + y_error[i]\n",
    "    return y, y_error\n",
    "def generate_y_true_1(coef, x, t,c):\n",
    "    return c/(1 + np.exp( -((x.dot(coef))).dot(t)))\n",
    "\n",
    "\n",
    "def generate_true_ate(coef, c, m):\n",
    "    samples_x_est = generate_x(d_c, 100000)\n",
    "    all_combo = list(powerset(list(np.arange(1, m+1))))\n",
    "    est_dict = {}\n",
    "    for i in all_combo:\n",
    "        t = np.zeros(m+1)\n",
    "        t[0] = 1\n",
    "        t[i] = 1\n",
    "        base_est = np.mean(generate_y_true_1(coef, samples_x_est, t, c))\n",
    "        est_dict[str(t)] = base_est\n",
    "    return est_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for period in tqdm(range(10000)):\n",
    "    #coef = np.random.normal(1, 5, [d_c, m+1])\n",
    "    coef = np.random.uniform(-0.3, 0.5, [d_c, m+1])\n",
    "    c_true = np.random.uniform(10, 20)\n",
    "    a = generate_true_ate(coef, c_true, m)\n",
    "    max_value = max(a.values())\n",
    "    base = a[str(list(a.keys())[0])]\n",
    "    if max_value == base or max_value - base > 1:\n",
    "        continue\n",
    "    elif (min(a.values())- base)/(max_value - base) < -5:\n",
    "        print(max_value - base,(min(a.values())- base)/(max_value - base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fnn_asig(trainloader,testloader):\n",
    "    # =========================\n",
    "    # Model Training\n",
    "    # =========================\n",
    "\n",
    "    is_gpu = False\n",
    "    net = FNN_asig(d_c, m)\n",
    "    train_accuracy_list = []\n",
    "    test_accuracy_list = []\n",
    "    # Full training\n",
    "    #print(\"---------train---------\")\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    criterion = nn.MSELoss()\n",
    "    # for epoch in range(train_epochs):\n",
    "    for epoch in range(6):\n",
    "        for i, data_i in enumerate(trainloader, 0):\n",
    "            inputs, labels = data_i\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            l1_norm = sum(p.abs().sum() for p in net.parameters())\n",
    "            loss = criterion(outputs, labels) + reg_loss * l1_norm\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_accuracy = calculate_mse(trainloader, is_gpu,net)\n",
    "        test_accuracy = calculate_mse(testloader, is_gpu,net)\n",
    "        \n",
    "        train_accuracy_list.append(train_accuracy)\n",
    "        test_accuracy_list.append(test_accuracy)\n",
    "    #     if epoch % 100 == 0:\n",
    "        #print(f\"Iteration: {epoch} | Training MSE: {train_accuracy} | Test MSE: {test_accuracy}\")\n",
    "        if test_accuracy < test_thres and epoch > 6:\n",
    "            return net\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(X_inference, T_inference_full, Y_inference, X_model, T_model, Y_model):    \n",
    "    X_train, X_test, T_train_full, T_test_full, Y_train, Y_test = train_test_split(X_model, T_model, Y_model, test_size=0.3, random_state=42)  \n",
    "# Add a column of ones to T_train and T_test\n",
    "    # T_train_full = np.hstack([np.ones((T_train.shape[0], 1)), T_train])\n",
    "    # T_test_full = np.hstack([np.ones((T_test.shape[0], 1)), T_test])\n",
    "\n",
    "    # Combine with features\n",
    "    X_train_full = np.hstack([X_train, T_train_full])\n",
    "    X_test_full = np.hstack([X_test, T_test_full])\n",
    "    X_train_full = X_train_full.astype(np.float32)\n",
    "    X_test_full = X_test_full.astype(np.float32)\n",
    "\n",
    "\n",
    "    # Add a column of ones to T_inference\n",
    "    #T_inference_full = np.hstack([np.ones((T_inference.shape[0], 1)), T_inference])\n",
    "\n",
    "    # Combine with features\n",
    "    full_data_est = np.hstack([X_inference, T_inference_full])\n",
    "    full_data_est = np.append(full_data_est, Y_inference.reshape(X_inference.shape[0],1), axis=1)\n",
    "    \n",
    "    # =========================\n",
    "    # Prepare PyTorch datasets\n",
    "    # =========================\n",
    "\n",
    "    opt = parser.parse_args()\n",
    "    trainset = torch.utils.data.TensorDataset(torch.Tensor(X_train_full), torch.Tensor(Y_train))\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=opt.batch_size_train, shuffle=True)\n",
    "\n",
    "\n",
    "    testset = torch.utils.data.TensorDataset(torch.Tensor(X_test_full), torch.Tensor(Y_test))\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=opt.batch_size_test, shuffle=False)\n",
    "\n",
    "    # =========================\n",
    "    # Training\n",
    "    # =========================\n",
    "    net = train_fnn_asig(trainloader,testloader)\n",
    "    # =========================\n",
    "    # Inference\n",
    "    # =========================\n",
    "    activation = {}\n",
    "    net.layer1.register_forward_hook(get_activation('layer1',activation))\n",
    "\n",
    "\n",
    "    # Inference\n",
    "    # =============================================\n",
    "\n",
    "\n",
    "    com_ate = []\n",
    "\n",
    "    results_list = []\n",
    "\n",
    "    for t_star in t_star_all:\n",
    "        start_time = time.time()\n",
    "        data_est = pd.DataFrame(full_data_est, columns = feature_list + t_list + ['y'])\n",
    "        #at t=0\n",
    "        t_star_base = t_star.copy()\n",
    "        x_all_set=np.float32(data_est)[:, 0:-1]\n",
    "        y_all_set=np.float32(data_est)[:, -1]\n",
    "        \n",
    "        allset = torch.utils.data.TensorDataset(torch.Tensor(x_all_set), torch.Tensor(y_all_set)) # create your datset\n",
    "        allloader = torch.utils.data.DataLoader(\n",
    "            allset, batch_size=1000, shuffle=False)\n",
    "        \n",
    "        beta_ = []\n",
    "        pred_y_loss = []\n",
    "        with torch.no_grad():\n",
    "            for i, data_ in enumerate(allloader, 0):\n",
    "                inputs, labels = data_\n",
    "                inputs, labels = Variable(inputs), Variable(labels)\n",
    "                outputs = net(inputs)\n",
    "                beta_.extend(activation['layer1'].tolist())  \n",
    "                pred_y_loss.extend(outputs.tolist())\n",
    "        beta_ = np.array(beta_).reshape(data_est.shape[0], m+1)\n",
    "        pred_y_loss = np.array(pred_y_loss).reshape(data_est.shape[0])\n",
    "        real_y = Y_inference.copy() \n",
    "        \n",
    "        for j in range(m):\n",
    "            x_all_set[:, -m+j] = t_star_base[j+1] \n",
    "        allset = torch.utils.data.TensorDataset(torch.Tensor(x_all_set), torch.Tensor(y_all_set)) # create your datset\n",
    "        allloader = torch.utils.data.DataLoader(\n",
    "            allset, batch_size=1000, shuffle=False)\n",
    "        pred_y = []\n",
    "        with torch.no_grad():\n",
    "            for i, data_ in enumerate(allloader, 0):\n",
    "                inputs, labels = data_\n",
    "                inputs, labels = Variable(inputs), Variable(labels)\n",
    "                outputs = net(inputs)\n",
    "                pred_y.extend(outputs.tolist())\n",
    "        pred_y = np.array(pred_y).reshape(data_est.shape[0])\n",
    "        \n",
    "\n",
    "        lambda_inv = []\n",
    "        G_theta = []\n",
    "        G_theta_loss = []\n",
    "        cnt = 0\n",
    "\n",
    "        for beta_temp in beta_:\n",
    "            \n",
    "            lambda_ = np.zeros([m+1, m+1])#asig use m+2\n",
    "            u_ = beta_temp.dot(t_star_base)\n",
    "            G_theta.append(np.array(t_star_base))\n",
    "            # G_theta是一个 ooo*5 的list\n",
    "\n",
    "            u_ = beta_temp.dot(T_inference_full[cnt])\n",
    "            G_theta_loss.append(np.array(T_inference_full[cnt]))\n",
    "            # G_theta_loss 也是一个 ooo*5 的list 和G_theta的区别是用的是真实Treatment的情况而不是t_start_base\n",
    "\n",
    "            cnt += 1\n",
    "\n",
    "            for i in range(t_star_all.shape[0]):\n",
    "                #y = avg_y(beta_temp, c_true, d_true, t)\n",
    "                t = t_star_all[i]\n",
    "                u = beta_temp.dot(t)\n",
    "                G_prime = np.array(t)\n",
    "                lambda_ += t_dist_obs[i]*2*np.outer(G_prime, G_prime)\n",
    "\n",
    "            try:\n",
    "                lambda_inv.append(inv(lambda_ + reg_term*np.eye(m+1)))\n",
    "            except:\n",
    "                print('Singular matrix')\n",
    "                \n",
    "\n",
    "        lambda_inv_loss_prime = []\n",
    "        for i in range(data_est.shape[0]):\n",
    "            lambda_inv_loss_prime.append(2*(pred_y_loss[i]-real_y[i])*lambda_inv[i].dot(G_theta_loss[i]))\n",
    "        lambda_inv_loss_prime = np.array(lambda_inv_loss_prime)\n",
    "\n",
    "        pred_y_debiased = []\n",
    "        for i in range(data_est.shape[0]):\n",
    "            pred_y_debiased.append(pred_y[i]-G_theta[i].dot(lambda_inv_loss_prime[i]))\n",
    "        results_list.append(pred_y_debiased)\n",
    "        #print(len(pred_y_debiased))  \n",
    "\n",
    "    return np.array(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Exp_time = 1000\n",
    "for std in [3,4,5]:\n",
    "    #std = 5\n",
    "    optimal_ate = []\n",
    "    results_list_N = []\n",
    "    for period in tqdm(range(Exp_time)):\n",
    "        #coef = np.random.normal(1, 5, [d_c, m+1])\n",
    "        coef = np.random.uniform(-0.3, 0.5, [d_c, m+1])\n",
    "        c_true = np.random.uniform(10, 20)\n",
    "        a = generate_true_ate(coef, c_true, m)\n",
    "        optimal_ate.append(a)\n",
    "        # Generate samples\n",
    "        X_sample =  generate_x(d_c, sample_size)\n",
    "        T_sample = generate_t(t_star_all, t_dist_obs, sample_size)\n",
    "        Y_sample, Y_error = generate_y_true(coef, c_true, 0, X_sample, T_sample, sample_size,std)\n",
    "\n",
    "        # Split data into training, testing  and inference sets\n",
    "        X_model, X_inference, T_model, T_inference, Y_model, Y_inference = train_test_split(X_sample, T_sample, Y_sample, test_size=0.5, random_state=42)\n",
    "        \n",
    "        results1 = inference(X_inference, T_inference, Y_inference, X_model, T_model, Y_model)\n",
    "        results2 = inference(X_model, T_model, Y_model, X_inference, T_inference, Y_inference)\n",
    "        results = np.concatenate((results1, results2), axis = 1)\n",
    "        #print(results.shape)\n",
    "        results_list_N.append(results)\n",
    "    results_array_N = np.array(results_list_N)\n",
    "    np.save('Misspecification_DML_linear_results_array_{}_{}.npy'.format(m,std), results_array_N)\n",
    "    np.save('Misspecification_DML_linear_optimal_ate_{}_{}.npy'.format(m,std), optimal_ate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "from scipy.stats import norm,ttest_ind\n",
    "\n",
    "m = 4\n",
    "std = 5\n",
    "base_t = np.zeros(m+1)\n",
    "base_t[0] = 1\n",
    "#base_t = list(base_t)\n",
    "all_combo = list(powerset(list(np.arange(1, m+1))))\n",
    "t_star_all = []\n",
    "for i in all_combo:\n",
    "    t = np.zeros(m+1)\n",
    "    t[0] = 1\n",
    "    t[i] = 1\n",
    "    t_star_all.append(t)\n",
    "t_star_all = np.int16(t_star_all)\n",
    "\n",
    "\n",
    "alpha = 0.05\n",
    "Z = norm.ppf(1 - alpha/2)\n",
    "cost = np.zeros((3,1000))\n",
    "#optimal_ate_list = optimal_ate.copy()\n",
    "results_array_N = np.load('Misspecification_DML_linear_results_array_{}_{}.npy'.format(m,std))\n",
    "optimal_ate_list = np.load('Misspecification_DML_linear_optimal_ate_{}_{}.npy'.format(m,std), allow_pickle=True)\n",
    "\n",
    "for j in range(results_array_N.shape[0]):\n",
    "    F = optimal_ate_list[j]\n",
    "    #optimal_ate =  max(F.values()) - min(F.values())\n",
    "    optimal_ate =  max(F.values()) - F[str(base_t)]\n",
    "\n",
    "    base = results_array_N[j][0]\n",
    "    N1 = base.shape[0]\n",
    "\n",
    "    decision_dml = np.zeros(m+1)\n",
    "    decision_dml[0] = 1\n",
    "\n",
    "    decision_experiments = []\n",
    "    for idx,i in enumerate(t_star_all):\n",
    "        if np.sum(i) == 2:\n",
    "            decision_experiments.append(idx)\n",
    "    #print(decision_experiments)\n",
    "\n",
    "    tao_hat_nonlinear = np.zeros(len(t_star_all))\n",
    "    p_value_list_nonlinear = np.zeros(len(t_star_all))\n",
    "    for idx,i in enumerate(t_star_all):\n",
    "        group1 = results_array_N[j][idx]\n",
    "        diff_mean = group1.mean() - base.mean()\n",
    "        if np.std(group1) == 0 and np.std(base) == 0:\n",
    "            p_value = 0\n",
    "        else:\n",
    "            t_stat, p_value = ttest_ind(group1, base, equal_var = False) \n",
    "        tao_hat_nonlinear[idx] = diff_mean\n",
    "        p_value_list_nonlinear[idx] = p_value\n",
    "    \n",
    "    decision3 = np.intersect1d(np.argwhere(p_value_list_nonlinear<alpha), np.argwhere(tao_hat_nonlinear>0))\n",
    "    if decision3.size == 0:\n",
    "        decision_nonlinear = np.array(t_star_all[0]).astype(float)\n",
    "    else:\n",
    "        idx = np.argmax(tao_hat_nonlinear[decision3])\n",
    "        decision_nonlinear = np.array(t_star_all[decision3[idx]]).astype(float)\n",
    "    if optimal_ate == 0:\n",
    "        if np.sum(decision_nonlinear) == 1:\n",
    "            cost[2,j] = 1\n",
    "        else:\n",
    "            cost[2,j] = 0\n",
    "    else:\n",
    "        cost[2,j] = (F[str(decision_nonlinear)] - F[str(base_t)])/optimal_ate   \n",
    "\n",
    "\n",
    "\n",
    "    tao_hat = np.zeros(m)\n",
    "    variance = np.zeros(m)\n",
    "    p_value_list = np.zeros(m)\n",
    "    for idx,i in enumerate(decision_experiments):\n",
    "        group1 = results_array_N[j][i]\n",
    "        diff_mean = group1.mean() - base.mean()\n",
    "        if np.std(group1) == 0 and np.std(base) == 0:\n",
    "            p_value = 0\n",
    "        else:\n",
    "            t_stat, p_value = ttest_ind(group1, base, equal_var = False) \n",
    "        tao_hat[idx] = diff_mean\n",
    "        p_value_list[idx] = p_value\n",
    "        variance[idx] = N1*(group1.var(ddof=1) / len(group1) + base.var(ddof=1) / len(base))\n",
    "    \n",
    "        \n",
    "    decision1 = np.intersect1d(np.argwhere(p_value_list<alpha), np.argwhere(tao_hat>0))\n",
    "    for i in decision1:\n",
    "        decision_dml[i + 1] = 1\n",
    "    if optimal_ate == 0:\n",
    "        if np.sum(decision_dml) == 1:\n",
    "            cost[0,j] = 1\n",
    "        else:\n",
    "            cost[0,j] = 0\n",
    "    else:\n",
    "        cost[0,j] = (F[str(decision_dml)] - F[str(base_t)])/optimal_ate   \n",
    "\n",
    "    tao_0 = np.mean(tao_hat)\n",
    "    numerator = np.mean(variance)\n",
    "    denumerator = np.mean((tao_hat - tao_0)**2) - numerator/N1\n",
    "    \n",
    "    if tao_0 == 0 or denumerator == 0:\n",
    "        beta = 0\n",
    "    else:\n",
    "        beta = numerator/denumerator + Z*np.sqrt(N1*numerator)/tao_0\n",
    "    \n",
    "    beta = max(0,beta)\n",
    "    #print(beta)\n",
    "    theta = N1/(N1+beta)\n",
    "    tao_shunken_hat = np.zeros(m)\n",
    "    p_value_list_shrunken = np.zeros(m)\n",
    "    for idx,i in enumerate(decision_experiments):\n",
    "        group_11 = theta*results_array_N[j][i] + (1-theta)*tao_0\n",
    "        group_00 = theta*base\n",
    "        if np.std(group_11) == 0 and np.std(group_00) == 0:\n",
    "            p_value = 0\n",
    "        else:\n",
    "            t_stat, p_value = ttest_ind(group_11, group_00, equal_var = False)\n",
    "\n",
    "        diff_mean1 = group_11.mean() - group_00.mean()\n",
    "        tao_shunken_hat[idx] = diff_mean1\n",
    "        p_value_list_shrunken[idx] = p_value\n",
    "    \n",
    "    decision_dml_shrunken = np.zeros(m + 1)\n",
    "    decision_dml_shrunken[0] = 1\n",
    "    decision2 = np.intersect1d(np.argwhere(p_value_list_shrunken<alpha), np.argwhere(tao_shunken_hat>0))\n",
    "    for i in decision2:\n",
    "        decision_dml_shrunken[i+1] = 1\n",
    "    if optimal_ate == 0:\n",
    "        if np.sum(decision_dml_shrunken) == 1:\n",
    "            cost[1,j] = 1\n",
    "        else:\n",
    "            cost[1,j] = 0\n",
    "    else:\n",
    "        cost[1,j] = (F[str(decision_dml_shrunken)] -  F[str(base_t)])/optimal_ate    \n",
    "#np.save(\"DML_nonlinear_cost.npy\", cost)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
